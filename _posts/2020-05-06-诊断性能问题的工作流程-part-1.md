---
layout: post
title: 诊断性能问题的工作流程 – Part 1（翻译计划中）
date: 2020-05-06 22:05 +0800
---

Creater:        [@Maoni Stephens](https://twitter.com/maoni0)

Translator:     @Murphy Shaun

Proofreader：待定

> [原文链接](https://devblogs.microsoft.com/dotnet/work-flow-of-diagnosing-memory-performance-issues-part-1)

---
# 作者授权

![authorize]({{ site.baseurl }}/images/1587561145552-0d8a560c-3b7d-443a-badc-a98ddbb6e7bf.png)

> 未经原作者允许以及保护隐私，这里将maoni大佬的邮箱地址隐去了，如果想要与原作者取得联系，请在原文的下方进行留言。

## 译者按

如果这篇文章可以帮到您，那么这将是我最大的一份荣幸，但也请您点进原文，给原作者一个善意的回复，或在文章下方留下善意的回复，您的支持将是这些可敬的社区磐石保持创作激情中最大的一部分:)

<b>中文版本将不会以任何形式收费，所有版权归属与原作者</b>

原作者Twitter：[https://twitter.com/maoni0](https://twitter.com/maoni0)

原作者Github：[https://github.com/Maoni0](https://github.com/Maoni0)

---
## 正文

在此篇文章中,我将讨论一点有关为PerfView做出贡献的内容,然后继续进行GCStats分析. 如果您喜欢,您可以直接[跳转到分析部分](https://devblogs.microsoft.com/dotnet/work-flow-of-diagnosing-memory-performance-issues-part-1/#continuing-the-analysis).

有一个令我沮丧的事情,市面上有很多的内存性能工具,但很少有针对通用类型以及与我所服务的客户的.所有的工具都很基础,很难进行中级和高级分析.
我知道PerfView在可用性上遭到了很多抱怨-我确实认为有一些抱怨是正确的.尽管如此,我仍然喜欢PerfView,因为这是我唯一一个可以用于完成工作的工具.

我希望人们可以理解

1) 我们放在PerfView上的精力非常有限.
  我们没有完整的工作团队 像是Visual Studio org;我们只有部分时间,来自于少部分成员的兼职,因此很难满足几乎所有用户的要求.
2) 在进行高级分析时.since it can be so diverse,这自然意味着可用性没有那么简单-当有很多需要查看的事情时,Permutation 就会很快的变得十分庞大.


 对类似PerfView之类的项目做出贡献,是对`.NET Core`做出贡献的绝佳方法,他没有Runtime本身那么陡峭的学习曲线,但是您的贡献会潜在的帮助人们节省大量时间. 您可以从克隆[repo](https://github.com/microsoft/perfview/) 并编译它开始. 然后您可以单步执行代码了 – IMO 如果您可以单步执行代码, 这始终是最好的了解新鲜事物的方法. 影响我在此讨论内容的代码大部分都位于2个文件中 – src\TraceEvent\Computers\TraceManagedProcess.cs and src\PerfView\GcStats.cs. 如果您搜索诸如 Clr.EventName (例如, Clr.GCStart, Clr.GCStop), 这也就是进行事件分析的地方 (您不需要关心对于trace的解析 – 这是在其他地方处理的). 所以对于GC的分析就是这个文件中的[GLAD](https://devblogs.microsoft.com/dotnet/glad-part-2/) (GC Latency Analysis and Diagnostics) 库. 和GcStats.cs 使用它来显示您在GCStats视图中所看的HTML文件. 如果您想在自己的工具上展示GC的相关信息,GCStats.cs将是一个很好的使用GLAD的例子.

### 继续分析

在上一篇文章,我们讨论了关于收集GCCollectOnly trace并在PerfView中启用了GC事件集合以及检查了GCStats视图.

我应该提醒您,在Linux上可以使用dotnet-trace. 根据[这个文档](https://github.com/dotnet/diagnostics/blob/master/documentation/dotnet-trace-instructions.md): 在这个构建的介绍中,提供了与/GCCollectOnly arg 等效的 PerfView’s 收集命令:

> 译者注:上文中的profile,指的是dotnet-trace命令的 --profile参数
```
 --profile

   [omitted]

   gc-collect   仅跟踪收集GC以极低的性能开销

```

您可以使用dotnet-trace命令,在Linux上收集跟踪.

<span style="color:red">dotnet trace collect -p <pid> -o <outputpath> --profile gc-collect</span>

然后在Windows上用PerfView进行展示. 当您查看GCStats视图时,唯一的不同来自于用户的POV:在Windows进行收集跟踪,您可以看到所有的托管线程.而在Linux下收集跟踪则只包含您指定线程的PID.

在这篇文章中,我将重点介绍您看到的GCStats表格. 我会展示一个例子. 流程的第一张表:“GC Rollup By Generation” –

<style>
  table, th, td {
    border: 1px solid black;
  }
  </style>

![WorkFlow-GCRollupByGeneration]({{site.baseurl}}/images/WorkFlow-GCRollupByGeneration.png)

我忽略了`Alloc MB/MSec GC` 和 `Survived MB/MSec GC` 列 – 他们在我开始维护PerfView之前就已经存在了,最好将其修复让其更有意义, but I never got around to.

现在,如果您想进行通用分析,这通常意味着没有紧急的投诉,并且您只想查看一下是否有可以改进的地方,您可以从rollup表开始.

如果我们查看上一个表, 马上我们就会注意到gen2的平均中断时间比gen0/1大得多. 我们可以猜测`gen2s`可能没有经历过中断,因为`Max Peak MB(最大峰值MB)`大约为13GB,如果我们要遍历所有的内存,可能要花费167ms.所以,这些可能是后台GC(Background GC) 并且在rollup表下方的 “Gen 2 for pid: process_name” 表中得到了确认 (我删除了一些列,所以它不会特别宽) –

![WorkFlow-GCRollupByGeneration2]({{site.baseurl}}/images/WorkFlow-GCRollupByGeneration2.png)

2B 表示在后台执行的二代GC. 如果您想知道其他的一些组合, 您只需要将鼠标悬停在"Gen"的列标题上,您将看到以下文本:

<span style="color:red">N=NonConcurrent/非并发式GC, B=Background/后台GC, F=Foreground/前台GC (while background is running) I=Induced/触发式GC i=InducedNotForced/触发非前台</span>

所以对于gen2,您可能会看到2N, 2NI, 2Ni or 2Bi.如果您使用 GC.Collect来触发GC,它有两个采用此参数的重载 –

```

bool blocking

```

除非您将该参数指定为False,它意味着将始终以阻塞的方式触发GC.这就是为什么没有2BI的原因

在rollup table中,始终有一列`Induced`显示为0 , 尤其是当与GCs的总数相比时,如果一个相当大的数字,找出是谁在触发这些GCs是一个非常好的主意.这在 [这篇博客中做了详细的讲解](https://devblogs.microsoft.com/dotnet/gc-etw-events-2/).

所以,我们知道了这些GCs全部都是BGC,但是,对于BGC来说,这些中断的时间太长了! 请注意,虽然我将两次中断显示成了一次,但确实是由两次中断组成的.在[the picture](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/media/fundamentals/background-workstation-garbage-collection.png) MSDN的图片中,显示了一次BGC中的两次中断(蓝色的列所指的位置).

但是,您在GCStats中看到的中断时间是这两次中断合计.造成这个的原因,是因为最初的中断通常都非常短(图片中的蓝色列仅仅用于`illustration`目的-她们并不代表实际上真正用了多长时间). 在这种情况下,我们想看一下每一个单独的中断有多长时间 - 我正在考虑在GLAD中提供各个BGC的中断信息,但在这之前,this is how you can figure out for yourself.

在[这篇博客中](https://devblogs.microsoft.com/dotnet/gc-etw-events-3/),我描述了BGC的实际的事件顺序.所以 我们需要找到两次`SuspendEE/RestartEE`事件.

为此,您可以在Perfview中打开Events试图,然后从“Pause Start” 开始.

让我们以GC#9217为例,它的首次中断在789,274.32 您可以在“Start”输入框中输入它.然后过滤类型“gc/” 仅过滤GC事件,并选择SuspendEE/RestartEE/GCStart/GCStop事件,然后摁下回车.

下面是此时您将会看到的示例图片(处于隐私原因,我删除了进程名字) - 


![WorkFlow1-0]({{ site.baseurl }}/images/WorkFlow1-0.jpg)


如果您在第一个SuspendEEStart和第一个RestartEEStop上选择时间戳,这是第一次的中断.

我们可以看到这个视图的状态栏显示了两个时间戳的差异是 75.902.

这已经非常长了 - 一通常来说,初次的中断时间每一组都应当不超过几毫秒.

对于这种情况,您基本上可以将其交给我,因为这绝对不是设计使然.

但是,如果您有兴趣自己进行进一步的诊断,下一步将是捕获更多的事件Trace,来向我们展示在停顿期间发生了什么.

通常,我们都会捕获CPU样本事件+GC事件的跟踪.CPU样本清楚的向我们展示了真正的罪魁祸首.不是GC而是运行时中的其他东西.此后我们已经修复了,这个性能问题只有在您的程序中有多个模块时才会展示出来(在这个特殊的场景下,客户拥有数千个模块).

第二次的BGC中断开始与SuspendEEStart事件,原因是“SuspendForGCPrep”,与第一次的SuspendEESrart事件不同的是,此次原因是“SuspendForGC”.

当是由GC为目的引发的停顿,仅有两个可能,原因是“SuspendForGCPrep”仅在初次中断的BGC期间可用.

通常来说,一个BGC仅会有两次中断,但是如果您启用了`GCHeapSurvivalAndMovementKeyword`事件,您将在GBC期间添加第三个中断,因为要触发这些事件,托管线程必须处于中断状态.

如果是这种情况,第三个暂停也会有“SuspendForGCPerp”原因,并且通常比其他两个中断要长的多.因为如果堆很大,触发事件将花费很长的时间.

我已经看过很多次了,`when folks didn’t even need those events were seeing an artificially long pause for BGC due to this exact reason`.

您可能会问,为什么在不需要收集这些事件的时候,还会不小心的收集它们.这是因为在您收集运行时事件时,它们已经包含在默认值中(您可以在src\TraceEvent\Parsers\ClrTraceEventParser.cs中看到默认值中包含那些关键字,搜索default.您会看到由许多关键字被包含在了默认值中).

通常,我认为PerfView的原理是,默认情况下应当收集足够的事件提供给您全部的类别以便进行调查.总的来说,这是一个很好的策略因为您可能没有其他的的`repro`.

But you need to be able to tell what’s caused by collecting the events themselves and what’s due to the product. This of course assumes if you can afford to collect this many events. Sometimes it’s definitely not the case which is why I generally ask folks to start with lightweight tracing to indicate to us whether there is a problem and if so what other events we should collect.

Another thing we notice from the gen2 table is all of the BGCs were triggered by AllocLarge. Possible Trigger reasons are defined as GCReason in src\TraceEvent\Parsers\ClrTraceEventParser.cs:

``` java

public enum GCReason
{
    AllocSmall = 0x0,
    Induced = 0x1,
    LowMemory = 0x2,
    Empty = 0x3,
    AllocLarge = 0x4,
    OutOfSpaceSOH = 0x5,
    OutOfSpaceLOH = 0x6,
    InducedNotForced = 0x7,
    Internal = 0x8,
    InducedLowMemory = 0x9,
    InducedCompacting = 0xa,
    LowMemoryHost = 0xb,
    PMFullGC = 0xc,
    LowMemoryHostBlocking = 0xd
}

```

The most common reason is AllocSmall which means your allocation on the SOH triggered this GC. AllocLarge means an LOH allocation triggered this GC. In this particular case the team was already aware they were doing a lot of LOH allocations – they just didn’t know they caused BGCs this frequently. If you look at the “Gen2 Survival Rate %” column you’ll notice that the surv rate for gen2 is very high (97%) but the “LOH Survival Rate %” is very low – 29%. This tells us that there are a lot of LOH allocations that are fairly short lived.

We do adjust the LOH budget based on the gen2 budget so for cases like this we don’t triggered too many gen2 GCs. If we wanted to have LOH surv rate to be higher we’d need to trigger BGCs more often than this. If you know your LOH allocations are generally temporary a good thing to do is to make the LOH threshold larger via the GCLOHThreshold config.

That’s all for today. Next time we’ll talk more about tables in the GCStats view.






