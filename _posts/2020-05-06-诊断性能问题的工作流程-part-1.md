---
layout: post
title: 诊断性能问题的工作流程 – Part 1（翻译计划中）
date: 2020-05-06 22:05 +0800
---

Creater:        [@Maoni Stephens](https://twitter.com/maoni0)

Translator:     @Shaun Murphy

Proofreader：待定

> [原文链接](https://devblogs.microsoft.com/dotnet/work-flow-of-diagnosing-memory-performance-issues-part-0)

---
# 作者授权

![authorize]({{ site.baseurl }}/images/1587561145552-0d8a560c-3b7d-443a-badc-a98ddbb6e7bf.png)

> 未经原作者允许以及保护隐私，这里将maoni大佬的邮箱地址隐去了，如果想要与原作者取得联系，请在原文的下方进行留言。

## 译者按

如果这篇文章可以帮到您，那么这将是我最大的一份荣幸，但也请您点进原文，给原作者一个善意的回复，或在文章下方留下善意的回复，您的支持将是这些可敬的社区磐石保持创作激情中最大的一部分:)

<b>中文版本将不会以任何形式收费，所有版权归属与原作者</b>

原作者Twitter：[https://twitter.com/maoni0](https://twitter.com/maoni0)

原作者Github：[https://github.com/Maoni0](https://github.com/Maoni0)

---
## 正文

在此篇文章中,我将讨论一点有关为PerfView做出贡献的内容,然后继续进行GCStats分析. 如果您喜欢,您可以直接[跳转到分析部分](https://devblogs.microsoft.com/dotnet/work-flow-of-diagnosing-memory-performance-issues-part-1/#continuing-the-analysis).

有一个令我沮丧的事情,市面上有很多的内存性能工具,但很少有针对通用类型以及与我所服务的客户的.所有的工具都很基础,很难进行中级和高级分析.
我知道PerfView在可用性上遭到了很多抱怨-我确实认为有一些抱怨是正确的.尽管如此,我仍然喜欢PerfView,因为这是我唯一一个可以用于完成工作的工具.

我希望人们可以理解

1) 我们放在PerfView上的精力非常有限.
  我们没有完整的工作团队 像是Visual Studio org;我们只有部分时间,来自于少部分成员的兼职,因此很难满足几乎所有用户的要求.
2) 在进行高级分析时.since it can be so diverse,这自然意味着可用性没有那么简单-当有很多需要查看的事情时,Permutation 就会很快的变得十分庞大.


 对类似PerfView之类的项目做出贡献,是对`.NET Core`做出贡献的绝佳方法,他没有Runtime本身那么陡峭的学习曲线,但是您的贡献会潜在的帮助人们节省大量时间. 您可以从克隆[repo](https://github.com/microsoft/perfview/) 并编译它开始. 然后您可以单步执行代码了 – IMO 如果您可以单步执行代码, 这始终是最好的了解新鲜事物的方法. 影响我在此讨论内容的代码大部分都位于2个文件中 – src\TraceEvent\Computers\TraceManagedProcess.cs and src\PerfView\GcStats.cs. 如果您搜索诸如 Clr.EventName (例如, Clr.GCStart, Clr.GCStop), 这也就是进行事件分析的地方 (您不需要关心对于trace的解析 – 这是在其他地方处理的). 所以对于GC的分析就是这个文件中的[GLAD](https://devblogs.microsoft.com/dotnet/glad-part-2/) (GC Latency Analysis and Diagnostics) 库. 和GcStats.cs 使用它来显示您在GCStats视图中所看的HTML文件. 如果您想在自己的工具上展示GC的相关信息,GCStats.cs将是一个很好的使用GLAD的例子.

### 继续分析

在上一篇文章,我们讨论了关于收集GCCollectOnly trace并在PerfView中启用了GC事件集合以及检查了GCStats视图.

我应该提醒您,在Linux上可以使用dotnet-trace. 根据[这个文档](https://github.com/dotnet/diagnostics/blob/master/documentation/dotnet-trace-instructions.md): 在这个构建的介绍中,提供了与/GCCollectOnly arg 等效的 PerfView’s 收集命令:

> 译者注:上文中的profile,指的是dotnet-trace命令的 --profile参数
```
 --profile

   [omitted]

   gc-collect   仅跟踪收集GC以极低的性能开销

```

您可以使用dotnet-trace命令,在Linux上收集跟踪.

<span style="color:red">dotnet trace collect -p <pid> -o <outputpath> --profile gc-collect</span>

然后在Windows上用PerfView进行展示. 当您查看GCStats视图时,唯一的不同来自于用户的POV:在Windows进行收集跟踪,您可以看到所有的托管线程.而在Linux下收集跟踪则只包含您指定线程的PID.

在这篇文章中,我将重点介绍您看到的GCStats表格. 我会展示一个例子. 流程的第一张表:“GC Rollup By Generation” –

<style>
  table, th, td {
    border: 1px solid black;
  }
  </style>

![WorkFlow-GCRollupByGeneration]({{site.baseurl}}/images/WorkFlow-GCRollupByGeneration.png)

我忽略了`Alloc MB/MSec GC` 和 `Survived MB/MSec GC` 列 – 他们在我开始维护PerfView之前就已经存在了,最好将其修复让其更有意义, but I never got around to.

现在,如果您想进行通用分析,这通常意味着没有紧急的投诉,并且您只想查看一下是否有可以改进的地方,您可以从rollup表开始.

如果我们查看上一个表, 马上我们就会注意到gen2的平均中断时间比gen0/1大得多. 我们可以猜测`gen2s`可能没有经历过中断,因为`Max Peak MB(最大峰值MB)`大约为13GB,如果我们要遍历所有的内存,可能要花费167ms.所以,这些可能是后台GC(Background GC) 并且在rollup表下方的 “Gen 2 for pid: process_name” 表中得到了确认 (我删除了一些列,所以它不会特别宽) –

![WorkFlow-GCRollupByGeneration2]({{site.baseurl}}/images/WorkFlow-GCRollupByGeneration2.png)

2B 表示在后台执行的二代GC. 如果您想知道其他的一些组合, 您只需要将鼠标悬停在"Gen"的列标题上,您将看到以下文本:

<span style="color:red">N=NonConcurrent/非并发式GC, B=Background/后台GC, F=Foreground/前台GC (while background is running) I=Induced/触发式GC i=InducedNotForced/触发非前台</span>

所以对于gen2,您可能会看到2N, 2NI, 2Ni or 2Bi.如果您使用 GC.Collect来触发GC,它有两个采用此参数的重载 –

```

bool blocking

```

除非您将该参数指定为False,它意味着将始终以阻塞的方式触发GC.这就是为什么没有2BI的原因

在rollup table中,始终有一列`Induced`显示为0 , especially if this is a fairly significant number compared to the total number of GCs, it’s always a good idea to figure out who’s inducing these GCs. This is described in [this blog entry](https://devblogs.microsoft.com/dotnet/gc-etw-events-2/).

So we know these are all BGCs but for BGC these pauses are very long! Note that I show the pause for BGC as one pause but it really consists of 2 pauses. This picture from the GC MSDN page shows the 2 pauses during one BGC (where the blue arrows are). But the pause time you see in GCStats is the sum of these 2 pauses. The reason was the initial pause is usually very short (the arrows in [the picture](https://docs.microsoft.com/en-us/dotnet/standard/garbage-collection/media/fundamentals/background-workstation-garbage-collection.png) are merely for illustration purposes – they do not represent how long the time periods actually are). In this case we want to take a look how long each individual pauses are – I’m thinking to just provide the individual BGC pause info in GLAD but before that happens, this is how you can figure out for yourself.

In [this blog entry](https://devblogs.microsoft.com/dotnet/gc-etw-events-3/) I described the actual event sequence of a BGC. So we are really just looking for the 2 SuspendEE/RestartEE event pairs. To do this you can open the Events view in PerfView and start from the “Pause Start”. Let’s take GC#9217 as an example, its Pause Start is 789,274.32 which you can enter into the “Start” textbox. For Filter type “gc/” to filter to just the GC events, and select the SuspendEE/RestartEE/GCStart/GCStop events and press enter. Below is an example picture of what you would see at this point (I erased the process name for privacy reasons) –

![WorkFlow1-0]({{ site.baseurl }}/images/WorkFlow1-0.jpg)

If you select the timestamp for the 1st SuspendEEStart and the 1st RestartEEStop, this is the 1st pause. We can see the status bar of this view shows you the diff of these 2 timestamps is 75.902. That’s very long – in general the initial pause should be no longer than a couple/few ms. At this point you could basically hand this over to me because this is totally not by design. However, if you are interested to diagnose further on your own, the next step would be to capture a trace with more events to show us what’s going on during this suspension period. Usually we capture a trace with CPU sample events + GC events. The CPU samples showed us clearly what the culprit was which was not actually in GC and was in fact something else in the runtime which we have since fixed and this perf issue only shows up when you have many modules in your process (in this particular case the customer had a few thousand modules).

The 2nd pause for this BGC starts with a SuspendEEStart event whose reason is “SuspendForGCPrep”, different from the 1st SuspendEEStart whose reason is “SuspendForGC”. When suspension is done for GC purpose these are the only 2 possible reasons and the “SuspendForGCPrep” is only used during BGC after the initial pause. Normally there are only 2 pauses in one BGC but if you enable events with the GCHeapSurvivalAndMovementKeyword, you will be adding a 3rd pause during a BGC because in order to fire these events the managed threads have to be pauses. If that’s the case the 3rd pause would also have the “SuspendForGCPrep” reason and is usually much longer than the other 2 pauses because it takes long time to fire events if you have a big heap. I have seen this quite a few times when folks didn’t even need those events were seeing an artificially long pause for BGC due to this exact reason. You might ask why would someone accidently collect these events if they didn’t need them. It’s because these are included in the Default when you collect the runtime events (you can see which keywords Default includes in src\TraceEvent\Parsers\ClrTraceEventParser.cs, just search for default. And you can see there are many keywords included in Default). In general I think PerfView’s philosophy is the default should collect enough events for you to do all sorts of investigations. And in general this is a good policy as you may not have another repro. But you need to be able to tell what’s caused by collecting the events themselves and what’s due to the product. This of course assumes if you can afford to collect this many events. Sometimes it’s definitely not the case which is why I generally ask folks to start with lightweight tracing to indicate to us whether there is a problem and if so what other events we should collect.

Another thing we notice from the gen2 table is all of the BGCs were triggered by AllocLarge. Possible Trigger reasons are defined as GCReason in src\TraceEvent\Parsers\ClrTraceEventParser.cs:

``` java

public enum GCReason
{
    AllocSmall = 0x0,
    Induced = 0x1,
    LowMemory = 0x2,
    Empty = 0x3,
    AllocLarge = 0x4,
    OutOfSpaceSOH = 0x5,
    OutOfSpaceLOH = 0x6,
    InducedNotForced = 0x7,
    Internal = 0x8,
    InducedLowMemory = 0x9,
    InducedCompacting = 0xa,
    LowMemoryHost = 0xb,
    PMFullGC = 0xc,
    LowMemoryHostBlocking = 0xd
}

```

The most common reason is AllocSmall which means your allocation on the SOH triggered this GC. AllocLarge means an LOH allocation triggered this GC. In this particular case the team was already aware they were doing a lot of LOH allocations – they just didn’t know they caused BGCs this frequently. If you look at the “Gen2 Survival Rate %” column you’ll notice that the surv rate for gen2 is very high (97%) but the “LOH Survival Rate %” is very low – 29%. This tells us that there are a lot of LOH allocations that are fairly short lived.

We do adjust the LOH budget based on the gen2 budget so for cases like this we don’t triggered too many gen2 GCs. If we wanted to have LOH surv rate to be higher we’d need to trigger BGCs more often than this. If you know your LOH allocations are generally temporary a good thing to do is to make the LOH threshold larger via the GCLOHThreshold config.

That’s all for today. Next time we’ll talk more about tables in the GCStats view.






